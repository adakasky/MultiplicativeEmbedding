\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bengio2003neural,collobert2011natural,mikolov2013distributed,pennington2014glove}
\citation{conneau2017supervised}
\citation{deerwester1990indexing}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{conneau2017supervised}
\citation{deng2009imagenet}
\citation{conneau2017supervised}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{bowman2015large}
\citation{conneau2017supervised}
\citation{deng2009imagenet}
\citation{sharif2014cnn}
\citation{taigman2014deepface}
\citation{antol2015vqa}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{bengio2003neural}
\citation{mikolov2013distributed}
\citation{conneau2017supervised}
\citation{chung2014empirical}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Natural Language Inference Task}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Word Embedding Models}{3}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A generic architecture of learning word embeddings on SNLI in a supervised manner\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:model_arc}{{1}{3}{A generic architecture of learning word embeddings on SNLI in a supervised manner\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Pre-train Word Embeddings}{3}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}End-to-End Approach}{3}{subsubsection.3.2.2}}
\citation{bowman2015large}
\citation{dolan2004unsupervised}
\citation{mikolov2013distributed}
\citation{levy2014linguistic}
\citation{faruqui2016problems}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Entailment Model}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Datasets}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The SNLI Dataset}{4}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}The MSR Paraphrase Corpus}{4}{subsection.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Data samples.} The SNLI dataset has labels of entailment, contradiction and neutral. The MSR Paraphrase Corpus only has labels of true and false paraphrase.\relax }}{4}{table.caption.3}}
\newlabel{tab:data_sample}{{1}{4}{\textbf {Data samples.} The SNLI dataset has labels of entailment, contradiction and neutral. The MSR Paraphrase Corpus only has labels of true and false paraphrase.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Evaluation Method}{4}{subsection.5.1}}
\citation{kingma2014adam}
\citation{conneau2017supervised}
\citation{conneau2017supervised}
\citation{conneau2017supervised}
\newlabel{fig:train_loss}{{\caption@xref {fig:train_loss}{ on input line 187}}{5}{Results}{figure.caption.4}{}}
\newlabel{sub@fig:train_loss}{{}{5}{Results}{figure.caption.4}{}}
\newlabel{fig:dev_loss}{{\caption@xref {fig:dev_loss}{ on input line 192}}{5}{Results}{figure.caption.4}{}}
\newlabel{sub@fig:dev_loss}{{}{5}{Results}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and validation loss on SNLI dataset.\relax }}{5}{figure.caption.4}}
\newlabel{fig:loss}{{2}{5}{Training and validation loss on SNLI dataset.\relax }{figure.caption.4}{}}
\newlabel{fig:train_acc}{{\caption@xref {fig:train_acc}{ on input line 203}}{5}{Results}{figure.caption.5}{}}
\newlabel{sub@fig:train_acc}{{}{5}{Results}{figure.caption.5}{}}
\newlabel{fig:dev_acc}{{\caption@xref {fig:dev_acc}{ on input line 208}}{5}{Results}{figure.caption.5}{}}
\newlabel{sub@fig:dev_acc}{{}{5}{Results}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and validation acc on SNLI dataset.\relax }}{5}{figure.caption.5}}
\newlabel{fig:acc}{{3}{5}{Training and validation acc on SNLI dataset.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training Details}{5}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{5}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Dropout Impact}{5}{subsection.6.1}}
\bibstyle{unsrt}
\bibdata{report}
\bibcite{bengio2003neural}{{1}{}{{}}{{}}}
\bibcite{collobert2011natural}{{2}{}{{}}{{}}}
\bibcite{mikolov2013distributed}{{3}{}{{}}{{}}}
\bibcite{pennington2014glove}{{4}{}{{}}{{}}}
\bibcite{conneau2017supervised}{{5}{}{{}}{{}}}
\bibcite{deerwester1990indexing}{{6}{}{{}}{{}}}
\bibcite{deng2009imagenet}{{7}{}{{}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracies of SNLI development and test datasets, and accuracies of the transfer task MSR Pharaphrase Corpus. The dimension column indicates the number of hidden units of the sentence encoder. The performances of the first five models are provided by Conneau \emph  {et al. }\cite  {conneau2017supervised}.\relax }}{6}{table.caption.6}}
\newlabel{tab:acc}{{2}{6}{Accuracies of SNLI development and test datasets, and accuracies of the transfer task MSR Pharaphrase Corpus. The dimension column indicates the number of hidden units of the sentence encoder. The performances of the first five models are provided by Conneau \etal \cite {conneau2017supervised}.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Transferability}{6}{subsection.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{6}{section.7}}
\bibcite{bowman2015large}{{8}{}{{}}{{}}}
\bibcite{sharif2014cnn}{{9}{}{{}}{{}}}
\bibcite{taigman2014deepface}{{10}{}{{}}{{}}}
\bibcite{antol2015vqa}{{11}{}{{}}{{}}}
\bibcite{chung2014empirical}{{12}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{13}{}{{}}{{}}}
\bibcite{dolan2004unsupervised}{{14}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
