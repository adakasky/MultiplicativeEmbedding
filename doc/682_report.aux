\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{bengio2003neural}
\citation{collobert2011natural}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{conneau2017supervised}
\citation{deerwester1990indexing}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{conneau2017supervised}
\citation{deng2009imagenet}
\citation{conneau2017supervised}
\citation{bowman2015large}
\citation{conneau2017supervised}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\citation{deng2009imagenet}
\citation{sharif2014cnn}
\citation{taigman2014deepface}
\citation{antol2015vqa}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{bengio2003neural}
\citation{mikolov2013distributed}
\citation{conneau2017supervised}
\citation{chung2014empirical}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Approach}{2}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A generic architecture of learning word embeddings on SNLI in a supervised manner\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:model_arc}{{1}{2}{A generic architecture of learning word embeddings on SNLI in a supervised manner\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}The Natural Language Inference Task}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Word Embedding Models}{2}{subsection.3.2}}
\citation{bowman2015large}
\citation{dolan2004unsupervised}
\citation{conneau2017supervised}
\citation{conneau2017supervised}
\citation{mikolov2013distributed}
\citation{levy2014linguistic}
\citation{faruqui2016problems}
\citation{kingma2014adam}
\citation{conneau2017supervised}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Pre-train Word Embeddings}{3}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}End-to-End Approach}{3}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Entailment Model}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Datasets}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}The SNLI Dataset}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}The MSR Paraphrase Corpus}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Experiments}{3}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Evaluation Method}{3}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Training Details}{3}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Data samples.} The SNLI dataset has labels of entailment, contradiction and neutral. The MSR Paraphrase Corpus only has labels of true and false paraphrase.\relax }}{4}{table.caption.2}}
\newlabel{tab:data_sample}{{1}{4}{\textbf {Data samples.} The SNLI dataset has labels of entailment, contradiction and neutral. The MSR Paraphrase Corpus only has labels of true and false paraphrase.\relax }{table.caption.2}{}}
\newlabel{fig:train_loss}{{\caption@xref {fig:train_loss}{ on input line 133}}{4}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.3}{}}
\newlabel{sub@fig:train_loss}{{}{4}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.3}{}}
\newlabel{fig:dev_loss}{{\caption@xref {fig:dev_loss}{ on input line 138}}{4}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.3}{}}
\newlabel{sub@fig:dev_loss}{{}{4}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training and validation loss on SNLI dataset.\relax }}{4}{figure.caption.3}}
\newlabel{fig:loss}{{2}{4}{Training and validation loss on SNLI dataset.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Results}{4}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Dropout Impact}{4}{subsubsection.5.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Transferability}{4}{subsubsection.5.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusions}{4}{section.6}}
\bibstyle{unsrt}
\bibdata{report}
\bibcite{bengio2003neural}{1}
\bibcite{collobert2011natural}{2}
\bibcite{mikolov2013distributed}{3}
\bibcite{pennington2014glove}{4}
\bibcite{conneau2017supervised}{5}
\bibcite{deerwester1990indexing}{6}
\bibcite{deng2009imagenet}{7}
\newlabel{fig:train_acc}{{\caption@xref {fig:train_acc}{ on input line 149}}{5}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.4}{}}
\newlabel{sub@fig:train_acc}{{}{5}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.4}{}}
\newlabel{fig:dev_acc}{{\caption@xref {fig:dev_acc}{ on input line 154}}{5}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.4}{}}
\newlabel{sub@fig:dev_acc}{{}{5}{\hskip -1em.~The MSR Paraphrase Corpus}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training and validation acc on SNLI dataset.\relax }}{5}{figure.caption.4}}
\newlabel{fig:acc}{{3}{5}{Training and validation acc on SNLI dataset.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracies of SNLI development and test datasets, and accuracies of the transfer task MSR Pharaphrase Corpus. The dimension column indicates the number of hidden units of the sentence encoder. The performances of the first five models are provided by Conneau \emph  {et al}\onedot  \cite  {conneau2017supervised}.\relax }}{5}{table.caption.5}}
\newlabel{tab:acc}{{2}{5}{Accuracies of SNLI development and test datasets, and accuracies of the transfer task MSR Pharaphrase Corpus. The dimension column indicates the number of hidden units of the sentence encoder. The performances of the first five models are provided by Conneau \etal \cite {conneau2017supervised}.\relax }{table.caption.5}{}}
\bibcite{bowman2015large}{8}
\bibcite{sharif2014cnn}{9}
\bibcite{taigman2014deepface}{10}
\bibcite{antol2015vqa}{11}
\bibcite{chung2014empirical}{12}
\bibcite{hochreiter1997long}{13}
\bibcite{dolan2004unsupervised}{14}
\bibcite{levy2014linguistic}{15}
\bibcite{faruqui2016problems}{16}
\bibcite{kingma2014adam}{17}
